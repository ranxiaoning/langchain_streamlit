import streamlit as st
import os
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from io import BytesIO
from openai import OpenAI
from langchain.embeddings.base import Embeddings
from langchain.schema import BaseRetriever
from typing import List, Any
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import LLMResult
import threading
from typing import Dict, List, Any, Optional
import json


# è‡ªå®šä¹‰ Embeddings ç±»ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„ OpenAI å®¢æˆ·ç«¯
class CustomOpenAIEmbeddings(Embeddings):
    def __init__(self, client):
        self.client = client
        self.model = "text-embedding-3-small"  # å¯ä»¥æ ¹æ®éœ€è¦æ›´æ”¹æ¨¡å‹

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ä¸ºæ–‡æ¡£ç”ŸæˆåµŒå…¥å‘é‡"""
        try:
            response = self.client.embeddings.create(
                input=texts,
                model=self.model
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            st.error(f"ç”ŸæˆåµŒå…¥å‘é‡æ—¶å‡ºé”™: {e}")
            raise

    def embed_query(self, text: str) -> List[float]:
        """ä¸ºæŸ¥è¯¢ç”ŸæˆåµŒå…¥å‘é‡"""
        try:
            response = self.client.embeddings.create(
                input=[text],
                model=self.model
            )
            return response.data[0].embedding
        except Exception as e:
            st.error(f"ç”ŸæˆæŸ¥è¯¢åµŒå…¥å‘é‡æ—¶å‡ºé”™: {e}")
            raise


# è‡ªå®šä¹‰ LLM ç±»ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„ OpenAI å®¢æˆ·ç«¯
class CustomOpenAILLM:
    def __init__(self, client, model="gpt-4o-mini"):
        self.client = client
        self.model = model

    def __call__(self, prompt: str, stop: List[str] = None) -> str:
        """è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            return response.choices[0].message.content
        except Exception as e:
            st.error(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return f"é”™è¯¯: {str(e)}"


# æµå¼è¾“å‡ºå¤„ç†å™¨
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text
        self.lock = threading.Lock()

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        with self.lock:
            self.text += token
            self.container.markdown(self.text + "â–Œ")

    def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        with self.lock:
            self.container.markdown(self.text)


# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå¸ƒå±€
st.set_page_config(
    page_title="RAGæ–‡æ¡£é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ“š",
    layout="wide"
)

# æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ“š RAGæ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
st.markdown("ä¸Šä¼ æ‚¨çš„æ–‡æ¡£ï¼Œç„¶ååŸºäºæ–‡æ¡£å†…å®¹æé—®ã€‚ç³»ç»Ÿä¼šä»æ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯å¹¶ç”Ÿæˆç­”æ¡ˆã€‚")

# åˆå§‹åŒ–session state
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None
if "client" not in st.session_state:
    st.session_state.client = None

# ä¾§è¾¹æ  - APIå¯†é’¥è®¾ç½®å’Œæ–‡æ¡£ä¸Šä¼ 
with st.sidebar:
    st.header("è®¾ç½®")

    # APIå¯†é’¥å’ŒåŸºç¡€URLè¾“å…¥
    api_key = st.text_input("APIå¯†é’¥", type="password", value="")
    base_url = st.text_input("APIåŸºç¡€URL", value="https://ai.nengyongai.cn/v1")
    model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"])

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    if api_key and base_url:
        try:
            st.session_state.client = OpenAI(
                api_key=api_key,
                base_url=base_url
            )
            st.success("APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼")
        except Exception as e:
            st.error(f"åˆå§‹åŒ–APIå®¢æˆ·ç«¯å¤±è´¥: {e}")

    # æ–‡æ¡£ä¸Šä¼ 
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡æ¡£",
        type=["pdf", "txt"],
        accept_multiple_files=True
    )

    # å¤„ç†æŒ‰é’®
    process_btn = st.button("å¤„ç†æ–‡æ¡£")

# å¤„ç†æ–‡æ¡£
if process_btn and uploaded_files and st.session_state.client:
    with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
        try:
            documents = []

            # è¯»å–ä¸Šä¼ çš„æ–‡ä»¶
            for uploaded_file in uploaded_files:
                file_bytes = BytesIO(uploaded_file.read())

                if uploaded_file.type == "application/pdf":
                    # ä¿å­˜ä¸´æ—¶PDFæ–‡ä»¶
                    with open("temp.pdf", "wb") as f:
                        f.write(file_bytes.getvalue())
                    loader = PyPDFLoader("temp.pdf")
                    docs = loader.load()
                    documents.extend(docs)
                    os.remove("temp.pdf")

                elif uploaded_file.type == "text/plain":
                    # å¤„ç†æ–‡æœ¬æ–‡ä»¶
                    text = str(file_bytes.read(), "utf-8")
                    # ä½¿ç”¨æ–‡ä»¶å†…å®¹åˆ›å»ºTextLoader
                    with open("temp.txt", "w", encoding="utf-8") as f:
                        f.write(text)
                    loader = TextLoader("temp.txt")
                    docs = loader.load()
                    documents.extend(docs)
                    os.remove("temp.txt")

            # åˆ†å‰²æ–‡æ¡£
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            splits = text_splitter.split_documents(documents)

            # åˆ›å»ºè‡ªå®šä¹‰çš„embeddings
            embeddings = CustomOpenAIEmbeddings(st.session_state.client)

            # åˆ›å»ºå‘é‡å­˜å‚¨
            vector_store = FAISS.from_documents(splits, embeddings)

            # åˆ›å»ºè‡ªå®šä¹‰çš„LLM
            llm = CustomOpenAILLM(st.session_state.client, model=model_name)

            # åˆ›å»ºQAé“¾
            prompt_template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡ç‰‡æ®µæ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚
            å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
            å°½é‡ä½¿ç­”æ¡ˆè¯¦ç»†ä¸”å…¨é¢ã€‚

            {context}

            é—®é¢˜: {question}
            ç­”æ¡ˆ:"""

            PROMPT = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )


            # ç”±äºæˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰LLMï¼Œéœ€è¦æ‰‹åŠ¨å®ç°æ£€ç´¢å’Œç”Ÿæˆé€»è¾‘
            class CustomRetrievalQA:
                def __init__(self, retriever, llm, prompt_template):
                    self.retriever = retriever
                    self.llm = llm
                    self.prompt_template = prompt_template

                def __call__(self, query):
                    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
                    relevant_docs = self.retriever.get_relevant_documents(query)

                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n\n".join([doc.page_content for doc in relevant_docs])

                    # æ„å»ºæç¤ºè¯
                    prompt = self.prompt_template.format(context=context, question=query)

                    # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
                    answer = self.llm(prompt)

                    return {
                        "result": answer,
                        "source_documents": relevant_docs
                    }


            # åˆ›å»ºè‡ªå®šä¹‰çš„QAé“¾
            qa_chain = CustomRetrievalQA(
                retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
                llm=llm,
                prompt_template=PROMPT
            )

            # ä¿å­˜åˆ°session state
            st.session_state.vector_store = vector_store
            st.session_state.qa_chain = qa_chain

            st.success(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼å·²åŠ è½½ {len(splits)} ä¸ªæ–‡æœ¬ç‰‡æ®µã€‚")

        except Exception as e:
            st.error(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
elif process_btn:
    if not st.session_state.client:
        st.error("è¯·å…ˆé…ç½®APIå¯†é’¥å’ŒåŸºç¡€URL")
    elif not uploaded_files:
        st.error("è¯·ä¸Šä¼ è‡³å°‘ä¸€ä¸ªæ–‡æ¡£")

# èŠå¤©ç•Œé¢
if st.session_state.qa_chain:
    # æ˜¾ç¤ºèŠå¤©å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "sources" in message and message["sources"]:
                with st.expander("æŸ¥çœ‹æ¥æºæ–‡æ¡£"):
                    for i, source in enumerate(message["sources"]):
                        st.markdown(f"**æ¥æº {i + 1}:** {source.page_content[:200]}...")

    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # è·å–å›ç­”
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            with st.spinner("æ€è€ƒä¸­..."):
                try:
                    result = st.session_state.qa_chain(prompt)

                    # æµå¼è¾“å‡º
                    response_text = ""
                    for char in result["result"]:
                        response_text += char
                        message_placeholder.markdown(response_text + "â–Œ")

                    message_placeholder.markdown(response_text)

                    # æ˜¾ç¤ºæ¥æºæ–‡æ¡£ï¼ˆå¦‚æœæœ‰ï¼‰
                    if result["source_documents"]:
                        with st.expander("æŸ¥çœ‹æ¥æºæ–‡æ¡£"):
                            for i, doc in enumerate(result["source_documents"]):
                                st.markdown(f"**æ¥æº {i + 1}:** {doc.page_content[:200]}...")

                    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": response_text,
                        "sources": result["source_documents"]
                    })

                except Exception as e:
                    st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
else:
    st.info("ğŸ‘ˆ è¯·åœ¨ä¾§è¾¹æ é…ç½®APIä¿¡æ¯å¹¶ä¸Šä¼ æ–‡æ¡£ä»¥å¼€å§‹å¯¹è¯")

# é¡µè„š
st.markdown("---")
st.markdown("åŸºäºè‡ªå®šä¹‰OpenAI APIå’ŒLangChainæ„å»ºçš„RAGæ–‡æ¡£é—®ç­”ç³»ç»Ÿ")